{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ATTENTION LEARNING:**\n",
        "\n",
        "Here in this part we are going to cover attention learning. A classical example of such is the **Transformer**.\n",
        "\n",
        "In attention learning here we are putting the input and then a corresponding input embedding is passed which is then pass to the encoder and then to the decoder and as a result an output is generated.\n",
        "\n",
        "\n",
        "There are two main blocks:\n",
        "\n",
        "-    <font color='green'>**Encoder :** </font> \n",
        "\n",
        "Now what goes inside the Encoding layer. Here we have Multihead Attention where we pass 3 inputs, namely key, query and value. Passing it to normalizationa and scaling. Then it is send through Feed Forward and again normalized. This whole block as Nx also called as transformer block.\n",
        "\n",
        "\n",
        "\n",
        "-    <font color='green'>**Decoder :** </font> \n",
        "\n",
        "Now defining the decoder block what we can see is that we have the values and keys passed from the encoder but the qurie is passed from **another multiheaded attention** as its input. Which are then passed to the multiheaded attention. As we can see the further part of this is very similar to encode block with another layer of Multihead Attention and Normalization. The output is passed to the output encoder then to the block of decoder on the right hand side of the diagram. Lastly, Linear and Softmax are used for output probabilitites.\n",
        "\n",
        "This `decoder` and `encoder` block is repeated a couple of times. In encode block it will iterate multiple times before sending it to the decode.\n",
        "\n",
        "Position encoding encodes the position in each word(as we know changing the position changes the sentence contxt and meaning).\n",
        "\n",
        "\n",
        "**Link:** https://drive.google.com/file/d/1Tqlud4Iw375egpDSIdElvbQCf4Qk4v4g/view?usp=share_link\n",
        "\n",
        "\n",
        "All operations are done parallely. We are actually providing mask so that each elements go for it target. The first element target only the first whereas the second can target both first and second.\n",
        "\n",
        "**Link:** https://drive.google.com/file/d/1D5Nw00kTNrOgDmqi5LIc3u6iYCtKitZT/view?usp=share_link\n",
        "\n",
        "Now digging deeper we can map the embedding input of dim=256 splitting it into several parts(8 part then 32 dims each) and they all are passed through linear layers. Here we are sending the splitted input and then to the **Scaled Dot Product Attention**. Then to Concatination and Linear function made up our multihead attention(dim 256)\n",
        "\n",
        "**Link:** https://drive.google.com/file/d/1J1ZVixqTttVoRJHls_1aweK7SyCPGw8J/view?usp=share_link\n"
      ],
      "metadata": {
        "id": "D_e3MsVoRfmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a_viQasyRYcG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Self_attention(nn.Module): #base class for all neural networks helps to define, initialize and manipulate the parameters\n",
        "  def __init__(self,emb_size,heads): #we have embedding and we are going to split in different part mostly 8 and how many part we split it we call it heads\n",
        "    super(Self_attention,self).__init__()\n",
        "    self.emb_size=emb_size\n",
        "    self.heads=heads\n",
        "    self.head_dim=emb_size//heads  #doing the integer division\n",
        "\n",
        "    #as it should be in interger we cannot split 256 into 7 parts so for that case\n",
        "    assert (self.head_dim*heads==emb_size), \"Embedded size needs to be divided by heads\"\n",
        "\n",
        "    self.keys=nn.Linear(self.head_dim,self.head_dim,bias=False)#for linear transformation for sending our values keys and queries through\n",
        "    self.values=nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "    self.queries=nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "    self.fc_out=nn.Linear(heads*self.head_dim,emb_size) #fully connected out heads*self.head_dim needs to be equal to emb_size\n",
        "\n",
        "  def forward(self,values,keys,queries,mask):\n",
        "    N=queries.shape[0]  #for getting training examples and telling how many examples we are setting at the same time. Batch size\n",
        "    values_len,keys_len,queries_len= values.shape[1],keys.shape[1],queries.shape[1]# they always going to correspond to source len and taget len. Here we are making it abstract as we dont know where we are going to use it(can be in encoder/decoder).\n",
        "\n",
        "    #now splitting the embedding into self.heads no. of pieces\n",
        "    #this is done for capturing more complex relation to the input at different level and then passing each head throught attention mechanism to get the score which is \n",
        "    #then concatenated and use to compute the weighted sum of input embedding.\n",
        "    values=values.reshape(N,values_len,self.heads,self.head_dim) #we are splitting using self.heads,self.head_dim. if not done then we end up with no trainable parameters in the attention block\n",
        "    queries=queries.reshape(N,queries_len,self.heads,self.head_dim) \n",
        "    keys=keys.reshape(N,keys_len,self.heads,self.head_dim) \n",
        "\n",
        "    #now defining the score function\n",
        "    energy=torch.einsum(\"nqhd,nkhd-->nhqk\",[queries,keys]) #einsum performs einstein summation conventions that help in performing various complex operztion over the tensor\n",
        "    #querie shape: N,queries_len,self.heads,self.head_dim\n",
        "    #key shape: N,keys_len,self.heads,self.head_dim\n",
        "    #we finally get score shape= N, head, queries_len, keys_len\n",
        "    #queries_len is the target source sentence and key_len is the source sentence. For each word in our target how much we can attention on input.\n",
        "\n",
        "    if mask is not None:\n",
        "      energy=energy.mask_filled(mask==0,float(\"-1e20\")) #if the elements of mask is zero then we are going to shut this down so that it cannot impact.\n",
        "      #mask for the target is traingular matrix\n",
        "    #now with the help of softmax we are calculating the attention\n",
        "    attention=torch.softmax(energy/(self.emb_size**(1/2)),dim=3) #doing this for nemerical stability and also normalizing around key length\n",
        "    out=torch.einsum(\"nqhl,nlhd-->nqhd\",[attention,values]).reshape(  #for concatenation\n",
        "        N,queries_len, self.heads*self.head_dim\n",
        "    )\n",
        "    # attention shape: N, heads, queries_len, keys_len\n",
        "    # value shape: N,values_len,self.heads,self.head_dim\n",
        "    #out shape/ after einsum: N,queries_len,heads,head_dim   ; key _len=value_len  then faltten last two dimensions\n",
        "    out=self.fc_out(out) #the fully connected layer of the NN \n",
        "    return out"
      ],
      "metadata": {
        "id": "O79XkeDYOz0h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the attention formula follow the link:\n",
        "[Attention](https://drive.google.com/file/d/1SIG7SfJDG9HnmNp_yZmeP8x6_MnA9wlE/view?usp=share_link)"
      ],
      "metadata": {
        "id": "WABMnIO_z5IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import dropout\n",
        "# Now creating the transformer part\n",
        "\n",
        "class TransformerBlock(nn.Module): #base class for all neural networks helps to define, initialize and manipulate the parameters\n",
        "  def __init__(self,emb_size,heads,droupout,forward_expansion): #we have embedding and we are going to split in different part mostly 8 and how many part we split it we call it heads\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.attention=Self_attention(emb_size,heads)\n",
        "\n",
        "    #now passing it through the normalization. 2 types: 1st through the attention block then to normalization and after that 2nd through the feedforward and then normalization\n",
        "    self.norm1=nn.LayerNorm(emb_size) \n",
        "    self.norm2=nn.LayerNorm(emb_size)\n",
        "    self.feed_forward=nn.sequential(\n",
        "        nn.Linear(emb_size,forward_expansion*emb_size),           #here we are mapping it\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*emb_size,emb_size)            #mapping back to embed size\n",
        "    )\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,values,keys,queries,mask):\n",
        "    attention=self.attention(values,keys,queries,mask)\n",
        "    x=self.dropout(self.norm1(attention+queries))                                         #sending a skip connection \n",
        "    forward=self.feed_forward(x)\n",
        "    out=self.dropout(self.norm2(attention+x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "KPRX0Le9zt55"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the encoder\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_size,\n",
        "               heads,\n",
        "               droupout,\n",
        "               forward_expansion,\n",
        "               scr_vocab_size,\n",
        "               device,\n",
        "               num_layers,\n",
        "               max_length):    #positional embedding\n",
        "                     #declaration of hyper parameters\n",
        "    super(Encoder,self).__init__()\n",
        "    self.emb_size=emb_size\n",
        "    self.device=device\n",
        "    self.word_emb=nn.Embedding(scr_vocab_size,emb_size)\n",
        "    self.position_emb=nn.Embedding(max_length,emb_size)\n",
        "    self.layer=nn.ModuleList(                           #map different modules together\n",
        "        [\n",
        "            TransformerBlock(emb_size,heads,dropout=dropout,forward_expansion=forward_expansion) #for number of layers\n",
        "        ]\n",
        "    )\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,x,mask):    #awnding one input\n",
        "      N,seq_len=x.shape\n",
        "      positions=torch.arrange(0,seq_len).expand(N,seq_len).to(self.device)   #0---->seq_length for every example\n",
        "      #sending x through embedding\n",
        "      out=self.dropout(self.word_emb(x)+self.position_emb(positions))\n",
        "      for layers in self.layers:\n",
        "        out= layers(out,out,out,mask) #as the value,key and queries are equal.\n",
        "        return out"
      ],
      "metadata": {
        "id": "1U1xljpp5CD8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the decoder\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_size,\n",
        "               heads,\n",
        "               droupout,\n",
        "               forward_expansion,\n",
        "               device): \n",
        "    super(Encoder,self).__init__()\n",
        "    self.attention=Self_attention(emb_size,heads)\n",
        "    self.norm=nn.LayerNorm(emb_size)\n",
        "    self.TransformerBlock=TransformerBlock(emb_size,heads,droupout,forward_expansion)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self,x,value,key,scr_mask,trg_mask):\n",
        "    #taking i/p from target then value and key that we got from encoder\n",
        "    attention=self.attention(x,x,x,trg_mask)\n",
        "    query=self.dropout(self.norm(attention+x))\n",
        "    out=self.TransformerBlock(value,key,query,scr_mask)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "JUn1PuPM_hF1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               tar_vocab_size,\n",
        "               emb_size,\n",
        "               heads,\n",
        "               droupout,\n",
        "               forward_expansion,\n",
        "               device,\n",
        "               max_length,\n",
        "               num_layer): \n",
        "    super(Decoder,self).__init__()\n",
        "    self.emb_size=emb_size\n",
        "    self.device=device\n",
        "    self.word_emb=nn.Embedding(tar_vocab_size,emb_size)\n",
        "    self.position_emb=nn.Embedding(max_length,emb_size)\n",
        "    self.layer=nn.ModuleList(                           #map different modules together\n",
        "        [\n",
        "            DecoderBlock(emb_size,heads,dropout,forward_expansion,device) #for number of layers\n",
        "            for _ in range(num_layer)\n",
        "        ]\n",
        "    )\n",
        "    self.fc_out=nn.Linear(emb_size, tar_vocab_size) #this is the last linear layer that you will find in the diagram.\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "  def forward(self,x,scr_mask,trg_mask,enc_out):\n",
        "    N,seq_len=x.shape\n",
        "    positions=torch.arrange(0,seq_len).expand(N,seq_len).to(self.device)\n",
        "    x=self.dropout(self.word_emb(x)+self.position_emb(positions))\n",
        "    for layers in self.layers:\n",
        "      x= layers(x,enc_out,enc_out,scr_mask,trg_mask) #as the value that is input to the decoder block and enc_out is for the value of the values and keys\n",
        "    out=self.fc_out(x) #prediction of which word is next"
      ],
      "metadata": {
        "id": "uGikw-guK8bw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a mask of traingular matrix: [Triangular Matrix](https://drive.google.com/file/d/1GBjvR9KAdc13UwUiAJa2dBfJhJVypPAD/view?usp=share_link)"
      ],
      "metadata": {
        "id": "mEocYJcxV4L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Putting them together\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               tar_vocab_size,\n",
        "               scr_vocab_size,\n",
        "               tar_pad_index,     #for the mask\n",
        "               scr_pad_index,\n",
        "               emb_size=256,\n",
        "               num_layer=6,\n",
        "               forward_expansion=4,\n",
        "               heads=8,\n",
        "               dropout=0,\n",
        "               device=\"cuda\",\n",
        "               max_length=100\n",
        "               ):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.encoder=Encoder(\n",
        "        scr_vocab_size,\n",
        "        emb_size,\n",
        "        num_layer,\n",
        "        heads,\n",
        "        dropout,\n",
        "        max_length,\n",
        "        forward_expansion,\n",
        "        device\n",
        "    )\n",
        "    self.decoder=Decoder(\n",
        "        tar_vocab_size,\n",
        "        emb_size,\n",
        "        num_layer,\n",
        "        heads,\n",
        "        dropout,\n",
        "        max_length,\n",
        "        forward_expansion,\n",
        "        device\n",
        "    )\n",
        "    self.tar_pad_index=tar_pad_index\n",
        "    self.scr_pad_index=scr_pad_index\n",
        "    self.device=device\n",
        "\n",
        "  def make_scr_mask(self,scr):\n",
        "    scr_mask=(scr!=self.scr_pad_index).unsqueeze(1).unsqueeze(2) #if it is a source pad index then it will be 0 or else 1\n",
        "    #to shape it ---> (N,1,1,scr_len)\n",
        "    return scr_mask.to(self.device)\n",
        "\n",
        "  def make_tar_mask(self,tar):\n",
        "    N,tar_len=tar.shape\n",
        "    tar_mask=torch.tril(torch.ones((tar_len,tar_len))).expand(\n",
        "        N,1,tar_len,tar_len\n",
        "    )   #making a lower triangular matrix and also expanding to train for each\n",
        "    return tar_mask.to(self.device)\n",
        "\n",
        "  def forward(self,scr,tar):\n",
        "    scr_mask=self.make_scr_mask(scr)\n",
        "    tar_mask=self.make_tar_mask(tar)\n",
        "    enc_scr=self.encoder(scr,scr_mask)\n",
        "    out=self.encoder(tar,enc_scr,scr_mask,tar_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "PvFF49KKPc3b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
        "  tar= torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "  scr_pad_index = 0\n",
        "  tar_pad_index = 0\n",
        "  scr_vocab_size = 10\n",
        "  tar_vocab_size = 10\n",
        "  model = Transformer(scr_vocab_size, tar_vocab_size, scr_pad_index, tar_pad_index).to(device)\n",
        "  out = model(x, tar[:, :-1]) \n",
        "  print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "aA4nC8-XY9j8",
        "outputId": "f042c8fa-fed3-4077-cdba-224e7423a22c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b22960abf051>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mscr_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtar_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscr_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscr_pad_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_pad_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-27f67bd0aacd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tar_vocab_size, scr_vocab_size, tar_pad_index, scr_pad_index, emb_size, num_layer, forward_expansion, heads, dropout, device, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m                ):\n\u001b[1;32m     17\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     self.encoder=Encoder(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mscr_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0memb_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ef55dbf2180d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, emb_size, heads, droupout, forward_expansion, scr_vocab_size, device, num_layers, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscr_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     self.layer=nn.ModuleList(                           #map different modules together\n\u001b[1;32m     20\u001b[0m         [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n\u001b[0m\u001b[1;32m    143\u001b[0m                                     requires_grad=not _freeze)\n\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "44JcYTFNd0Sl"
      }
    }
  ]
}